"""
ç”Ÿæˆé›†æˆæ¨¡å—
"""

import os
import logging
from typing import List

from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_community.chat_models.moonshot import MoonshotChat
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

logger = logging.getLogger(__name__)

class GenerationIntegrationModule:
    """ç”Ÿæˆé›†æˆæ¨¡å— - è´Ÿè´£LLMé›†æˆå’Œå›ç­”ç”Ÿæˆ"""
    
    def __init__(self, model_name: str = "kimi-k2-0711-preview", temperature: float = 0.1, max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ç”Ÿæˆé›†æˆæ¨¡å—
        
        Args:
            model_name: æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.llm = None
        self.setup_llm()
    
    def setup_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹"""
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–LLM: {self.model_name}")

        api_key = os.getenv("MOONSHOT_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½® MOONSHOT_API_KEY ç¯å¢ƒå˜é‡")

        self.llm = MoonshotChat(
            model=self.model_name,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            moonshot_api_key=api_key
        )
        
        logger.info("LLMåˆå§‹åŒ–å®Œæˆ")
    
    def generate_basic_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”ŸæˆåŸºç¡€å›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹é£Ÿè°±ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³é£Ÿè°±ä¿¡æ¯:
{context}

è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

å›ç­”:""")

        # ä½¿ç”¨LCELæ„å»ºé“¾
        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query)
        return response
    
    def generate_step_by_step_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”Ÿæˆåˆ†æ­¥éª¤å›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åˆ†æ­¥éª¤çš„è¯¦ç»†å›ç­”
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªå¯¼å¸ˆã€‚è¯·æ ¹æ®é£Ÿè°±ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„åˆ†æ­¥éª¤æŒ‡å¯¼ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³é£Ÿè°±ä¿¡æ¯:
{context}

è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

## ğŸ¥˜ èœå“ä»‹ç»
[ç®€è¦ä»‹ç»èœå“ç‰¹ç‚¹å’Œéš¾åº¦]

## ğŸ›’ æ‰€éœ€é£Ÿæ
[åˆ—å‡ºä¸»è¦é£Ÿæå’Œç”¨é‡]

## ğŸ‘¨â€ğŸ³ åˆ¶ä½œæ­¥éª¤
[è¯¦ç»†çš„åˆ†æ­¥éª¤è¯´æ˜ï¼Œæ¯æ­¥åŒ…å«å…·ä½“æ“ä½œå’Œå¤§æ¦‚æ‰€éœ€æ—¶é—´]

## ğŸ’¡ åˆ¶ä½œæŠ€å·§
[ä»…åœ¨æœ‰å®ç”¨æŠ€å·§æ—¶åŒ…å«ã€‚ä¼˜å…ˆä½¿ç”¨åŸæ–‡ä¸­çš„å®ç”¨æŠ€å·§ï¼Œå¦‚æœåŸæ–‡çš„"é™„åŠ å†…å®¹"ä¸çƒ¹é¥ªæ— å…³æˆ–ä¸ºç©ºï¼Œå¯ä»¥åŸºäºåˆ¶ä½œæ­¥éª¤æ€»ç»“å…³é”®è¦ç‚¹ï¼Œæˆ–è€…å®Œå…¨çœç•¥æ­¤éƒ¨åˆ†]

æ³¨æ„ï¼š
- æ ¹æ®å®é™…å†…å®¹çµæ´»è°ƒæ•´ç»“æ„
- ä¸è¦å¼ºè¡Œå¡«å……æ— å…³å†…å®¹æˆ–é‡å¤åˆ¶ä½œæ­¥éª¤ä¸­çš„ä¿¡æ¯
- é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§
- å¦‚æœæ²¡æœ‰é¢å¤–çš„æŠ€å·§è¦åˆ†äº«ï¼Œå¯ä»¥çœç•¥åˆ¶ä½œæŠ€å·§éƒ¨åˆ†

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query)
        return response
    
    def query_rewrite(self, query: str) -> str:
        """
        æ™ºèƒ½æŸ¥è¯¢é‡å†™ - è®©å¤§æ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™æŸ¥è¯¢

        Args:
            query: åŸå§‹æŸ¥è¯¢

        Returns:
            é‡å†™åçš„æŸ¥è¯¢æˆ–åŸæŸ¥è¯¢
        """
        prompt = PromptTemplate(
            template="""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æŸ¥è¯¢åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é‡å†™ä»¥æé«˜é£Ÿè°±æœç´¢æ•ˆæœã€‚

åŸå§‹æŸ¥è¯¢: {query}

åˆ†æè§„åˆ™ï¼š
1. **å…·ä½“æ˜ç¡®çš„æŸ¥è¯¢**ï¼ˆç›´æ¥è¿”å›åŸæŸ¥è¯¢ï¼‰ï¼š
   - åŒ…å«å…·ä½“èœå“åç§°ï¼šå¦‚"å®«ä¿é¸¡ä¸æ€ä¹ˆåš"ã€"çº¢çƒ§è‚‰çš„åˆ¶ä½œæ–¹æ³•"
   - æ˜ç¡®çš„åˆ¶ä½œè¯¢é—®ï¼šå¦‚"è›‹ç‚’é¥­éœ€è¦ä»€ä¹ˆé£Ÿæ"ã€"ç³–é†‹æ’éª¨çš„æ­¥éª¤"
   - å…·ä½“çš„çƒ¹é¥ªæŠ€å·§ï¼šå¦‚"å¦‚ä½•ç‚’èœä¸ç²˜é”…"ã€"æ€æ ·è°ƒåˆ¶ç³–é†‹æ±"

2. **æ¨¡ç³Šä¸æ¸…çš„æŸ¥è¯¢**ï¼ˆéœ€è¦é‡å†™ï¼‰ï¼š
   - è¿‡äºå®½æ³›ï¼šå¦‚"åšèœ"ã€"æœ‰ä»€ä¹ˆå¥½åƒçš„"ã€"æ¨èä¸ªèœ"
   - ç¼ºä¹å…·ä½“ä¿¡æ¯ï¼šå¦‚"å·èœ"ã€"ç´ èœ"ã€"ç®€å•çš„"
   - å£è¯­åŒ–è¡¨è¾¾ï¼šå¦‚"æƒ³åƒç‚¹ä»€ä¹ˆ"ã€"æœ‰é¥®å“æ¨èå—"

é‡å†™åŸåˆ™ï¼š
- ä¿æŒåŸæ„ä¸å˜
- å¢åŠ ç›¸å…³çƒ¹é¥ªæœ¯è¯­
- ä¼˜å…ˆæ¨èç®€å•æ˜“åšçš„
- ä¿æŒç®€æ´æ€§

ç¤ºä¾‹ï¼š
- "åšèœ" â†’ "ç®€å•æ˜“åšçš„å®¶å¸¸èœè°±"
- "æœ‰é¥®å“æ¨èå—" â†’ "ç®€å•é¥®å“åˆ¶ä½œæ–¹æ³•"
- "æ¨èä¸ªèœ" â†’ "ç®€å•å®¶å¸¸èœæ¨è"
- "å·èœ" â†’ "ç»å…¸å·èœèœè°±"
- "å®«ä¿é¸¡ä¸æ€ä¹ˆåš" â†’ "å®«ä¿é¸¡ä¸æ€ä¹ˆåš"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰
- "çº¢çƒ§è‚‰éœ€è¦ä»€ä¹ˆé£Ÿæ" â†’ "çº¢çƒ§è‚‰éœ€è¦ä»€ä¹ˆé£Ÿæ"ï¼ˆä¿æŒåŸæŸ¥è¯¢ï¼‰

è¯·è¾“å‡ºæœ€ç»ˆæŸ¥è¯¢ï¼ˆå¦‚æœä¸éœ€è¦é‡å†™å°±è¿”å›åŸæŸ¥è¯¢ï¼‰:""",
            input_variables=["query"]
        )

        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        response = chain.invoke(query).strip()

        # è®°å½•é‡å†™ç»“æœ
        if response != query:
            logger.info(f"æŸ¥è¯¢å·²é‡å†™: '{query}' â†’ '{response}'")
        else:
            logger.info(f"æŸ¥è¯¢æ— éœ€é‡å†™: '{query}'")

        return response



    def query_router(self, query: str) -> str:
        """
        æŸ¥è¯¢è·¯ç”± - æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            è·¯ç”±ç±»å‹ ('list', 'detail', 'general')
        """
        prompt = ChatPromptTemplate.from_template("""
æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œå°†å…¶åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç§ç±»å‹ä¹‹ä¸€ï¼š

1. 'list' - ç”¨æˆ·æƒ³è¦è·å–èœå“åˆ—è¡¨æˆ–æ¨èï¼Œåªéœ€è¦èœå
   ä¾‹å¦‚ï¼šæ¨èå‡ ä¸ªç´ èœã€æœ‰ä»€ä¹ˆå·èœã€ç»™æˆ‘3ä¸ªç®€å•çš„èœ

2. 'detail' - ç”¨æˆ·æƒ³è¦å…·ä½“çš„åˆ¶ä½œæ–¹æ³•æˆ–è¯¦ç»†ä¿¡æ¯
   ä¾‹å¦‚ï¼šå®«ä¿é¸¡ä¸æ€ä¹ˆåšã€åˆ¶ä½œæ­¥éª¤ã€éœ€è¦ä»€ä¹ˆé£Ÿæ

3. 'general' - å…¶ä»–ä¸€èˆ¬æ€§é—®é¢˜
   ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯å·èœã€åˆ¶ä½œæŠ€å·§ã€è¥å…»ä»·å€¼

è¯·åªè¿”å›åˆ†ç±»ç»“æœï¼šlistã€detail æˆ– general

ç”¨æˆ·é—®é¢˜: {query}

åˆ†ç±»ç»“æœ:""")

        chain = (
            {"query": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        result = chain.invoke(query).strip().lower()

        # ç¡®ä¿è¿”å›æœ‰æ•ˆçš„è·¯ç”±ç±»å‹
        if result in ['list', 'detail', 'general']:
            return result
        else:
            return 'general'  # é»˜è®¤ç±»å‹

    def generate_list_answer(self, query: str, context_docs: List[Document]) -> str:
        """
        ç”Ÿæˆåˆ—è¡¨å¼å›ç­” - é€‚ç”¨äºæ¨èç±»æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Returns:
            åˆ—è¡¨å¼å›ç­”
        """
        if not context_docs:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„èœå“ä¿¡æ¯ã€‚"

        # æå–èœå“åç§°
        dish_names = []
        for doc in context_docs:
            dish_name = doc.metadata.get('dish_name', 'æœªçŸ¥èœå“')
            if dish_name not in dish_names:
                dish_names.append(dish_name)

        # æ„å»ºç®€æ´çš„åˆ—è¡¨å›ç­”
        if len(dish_names) == 1:
            return f"ä¸ºæ‚¨æ¨èï¼š{dish_names[0]}"
        elif len(dish_names) <= 3:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹èœå“ï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(dish_names)])
        else:
            return f"ä¸ºæ‚¨æ¨èä»¥ä¸‹èœå“ï¼š\n" + "\n".join([f"{i+1}. {name}" for i, name in enumerate(dish_names[:3])]) + f"\n\nè¿˜æœ‰å…¶ä»– {len(dish_names)-3} é“èœå“å¯ä¾›é€‰æ‹©ã€‚"

    def generate_basic_answer_stream(self, query: str, context_docs: List[Document]):
        """
        ç”ŸæˆåŸºç¡€å›ç­” - æµå¼è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            ç”Ÿæˆçš„å›ç­”ç‰‡æ®µ
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹é£Ÿè°±ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³é£Ÿè°±ä¿¡æ¯:
{context}

è¯·æä¾›è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in chain.stream(query):
            yield chunk

    def generate_step_by_step_answer_stream(self, query: str, context_docs: List[Document]):
        """
        ç”Ÿæˆè¯¦ç»†æ­¥éª¤å›ç­” - æµå¼è¾“å‡º

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context_docs: ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨

        Yields:
            è¯¦ç»†æ­¥éª¤å›ç­”ç‰‡æ®µ
        """
        context = self._build_context(context_docs)

        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çƒ¹é¥ªå¯¼å¸ˆã€‚è¯·æ ¹æ®é£Ÿè°±ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›è¯¦ç»†çš„åˆ†æ­¥éª¤æŒ‡å¯¼ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³é£Ÿè°±ä¿¡æ¯:
{context}

è¯·çµæ´»ç»„ç»‡å›ç­”ï¼Œå»ºè®®åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼ˆå¯æ ¹æ®å®é™…å†…å®¹è°ƒæ•´ï¼‰ï¼š

## ğŸ¥˜ èœå“ä»‹ç»
[ç®€è¦ä»‹ç»èœå“ç‰¹ç‚¹å’Œéš¾åº¦]

## ğŸ›’ æ‰€éœ€é£Ÿæ
[åˆ—å‡ºä¸»è¦é£Ÿæå’Œç”¨é‡]

## ğŸ‘¨â€ğŸ³ åˆ¶ä½œæ­¥éª¤
[è¯¦ç»†çš„åˆ†æ­¥éª¤è¯´æ˜ï¼Œæ¯æ­¥åŒ…å«å…·ä½“æ“ä½œå’Œå¤§æ¦‚æ‰€éœ€æ—¶é—´]

## ğŸ’¡ åˆ¶ä½œæŠ€å·§
[ä»…åœ¨æœ‰å®ç”¨æŠ€å·§æ—¶åŒ…å«ã€‚å¦‚æœåŸæ–‡çš„"é™„åŠ å†…å®¹"ä¸çƒ¹é¥ªæ— å…³æˆ–ä¸ºç©ºï¼Œå¯ä»¥åŸºäºåˆ¶ä½œæ­¥éª¤æ€»ç»“å…³é”®è¦ç‚¹ï¼Œæˆ–è€…å®Œå…¨çœç•¥æ­¤éƒ¨åˆ†]

æ³¨æ„ï¼š
- æ ¹æ®å®é™…å†…å®¹çµæ´»è°ƒæ•´ç»“æ„
- ä¸è¦å¼ºè¡Œå¡«å……æ— å…³å†…å®¹
- é‡ç‚¹çªå‡ºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§

å›ç­”:""")

        chain = (
            {"question": RunnablePassthrough(), "context": lambda _: context}
            | prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in chain.stream(query):
            yield chunk

    def _build_context(self, docs: List[Document], max_length: int = 2000) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not docs:
            return "æš‚æ— ç›¸å…³é£Ÿè°±ä¿¡æ¯ã€‚"
        
        context_parts = []
        current_length = 0
        
        for i, doc in enumerate(docs, 1):
            # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
            metadata_info = f"ã€é£Ÿè°± {i}ã€‘"
            if 'dish_name' in doc.metadata:
                metadata_info += f" {doc.metadata['dish_name']}"
            if 'category' in doc.metadata:
                metadata_info += f" | åˆ†ç±»: {doc.metadata['category']}"
            if 'difficulty' in doc.metadata:
                metadata_info += f" | éš¾åº¦: {doc.metadata['difficulty']}"
            
            # æ„å»ºæ–‡æ¡£æ–‡æœ¬
            doc_text = f"{metadata_info}\n{doc.page_content}\n"
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(doc_text) > max_length:
                break
            
            context_parts.append(doc_text)
            current_length += len(doc_text)
        
        return "\n" + "="*50 + "\n".join(context_parts)
